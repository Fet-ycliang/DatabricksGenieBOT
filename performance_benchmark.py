"""æ•ˆèƒ½å„ªåŒ–åŸºæº–æ¸¬è©¦å·¥å…·"""

import asyncio
import time
import statistics
import json
from typing import Dict, List
from datetime import datetime


class PerformanceBenchmark:
    """æ•ˆèƒ½åŸºæº–æ¸¬è©¦å·¥å…·"""
    
    def __init__(self):
        self.results = {
            'timestamp': datetime.now().isoformat(),
            'tests': {}
        }
    
    async def benchmark_logging(self, iterations: int = 1000) -> Dict:
        """åŸºæº–æ¸¬è©¦ï¼šæ—¥èªŒæ€§èƒ½"""
        import logging
        from io import StringIO
        
        # è¨­ç½®æ—¥èªŒ
        log_stream = StringIO()
        handler = logging.StreamHandler(log_stream)
        logger = logging.getLogger('test_logger')
        logger.addHandler(handler)
        logger.setLevel(logging.INFO)
        
        # æ¸¬è©¦ - é »ç¹æ—¥èªŒ
        durations = []
        for i in range(iterations):
            start = time.time()
            logger.info(f"Test message {i} with some data {{'key': 'value', 'number': {i}}}")
            durations.append(time.time() - start)
        
        result = {
            'test': 'logging_performance',
            'iterations': iterations,
            'metrics': {
                'mean_ms': statistics.mean(durations) * 1000,
                'median_ms': statistics.median(durations) * 1000,
                'p95_ms': sorted(durations)[int(iterations * 0.95)] * 1000,
                'p99_ms': sorted(durations)[int(iterations * 0.99)] * 1000,
            }
        }
        
        return result
    
    async def benchmark_json_serialization(self, iterations: int = 1000) -> Dict:
        """åŸºæº–æ¸¬è©¦ï¼šJSON åºåˆ—åŒ–"""
        import json
        
        test_data = {
            'user_id': '12345',
            'message': 'This is a test message with some content',
            'metadata': {
                'timestamp': datetime.now().isoformat(),
                'tags': ['tag1', 'tag2', 'tag3'],
                'nested': {
                    'level1': {
                        'level2': {
                            'level3': 'deep value'
                        }
                    }
                }
            },
            'large_data': [{'item': i, 'value': f'item_{i}'} for i in range(100)]
        }
        
        # æ¸¬è©¦ - é‡è¤‡åºåˆ—åŒ–
        durations = []
        for _ in range(iterations):
            start = time.time()
            json.dumps(test_data, indent=2)
            durations.append(time.time() - start)
        
        result = {
            'test': 'json_serialization',
            'iterations': iterations,
            'data_size_kb': len(json.dumps(test_data)) / 1024,
            'metrics': {
                'mean_us': statistics.mean(durations) * 1_000_000,
                'median_us': statistics.median(durations) * 1_000_000,
                'p95_us': sorted(durations)[int(iterations * 0.95)] * 1_000_000,
                'p99_us': sorted(durations)[int(iterations * 0.99)] * 1_000_000,
            }
        }
        
        return result
    
    async def benchmark_memory_operations(self, size: int = 10000) -> Dict:
        """åŸºæº–æ¸¬è©¦ï¼šå…§å­˜æ“ä½œ"""
        import sys
        
        # æ¸¬è©¦ - å­—å…¸æ“ä½œ
        test_dict = {}
        start = time.time()
        for i in range(size):
            test_dict[f'key_{i}'] = {
                'value': i,
                'data': f'x' * 100
            }
        dict_insert_time = time.time() - start
        
        # æ¸¬è©¦ - æŸ¥è©¢æ€§èƒ½
        start = time.time()
        for i in range(size):
            _ = test_dict.get(f'key_{i}', None)
        dict_lookup_time = time.time() - start
        
        result = {
            'test': 'memory_operations',
            'size': size,
            'metrics': {
                'dict_insert_ms': dict_insert_time * 1000,
                'dict_lookup_ms': dict_lookup_time * 1000,
                'memory_kb': sys.getsizeof(test_dict) / 1024,
            }
        }
        
        return result
    
    async def benchmark_string_operations(self, iterations: int = 10000) -> Dict:
        """åŸºæº–æ¸¬è©¦ï¼šå­—ç¬¦ä¸²æ“ä½œ"""
        
        # æ¸¬è©¦ - å­—ç¬¦ä¸²é€£æ¥ï¼ˆä½æ•ˆæ–¹å¼ï¼‰
        start = time.time()
        result = ""
        for i in range(iterations):
            result += f"Line {i}: Some text content\n"
        concat_time = time.time() - start
        
        # æ¸¬è©¦ - å­—ç¬¦ä¸²é€£æ¥ï¼ˆé«˜æ•ˆæ–¹å¼ï¼‰
        start = time.time()
        lines = [f"Line {i}: Some text content" for i in range(iterations)]
        result = "\n".join(lines)
        join_time = time.time() - start
        
        result_data = {
            'test': 'string_operations',
            'iterations': iterations,
            'metrics': {
                'concat_ms': concat_time * 1000,
                'join_ms': join_time * 1000,
                'improvement_percent': ((concat_time - join_time) / concat_time) * 100,
            }
        }
        
        return result_data
    
    async def benchmark_list_operations(self, size: int = 10000) -> Dict:
        """åŸºæº–æ¸¬è©¦ï¼šåˆ—è¡¨æ“ä½œ"""
        from collections import deque
        
        # æ¸¬è©¦ 1 - åˆ—è¡¨ pop(0) - ä½æ•ˆ
        test_list = list(range(size))
        start = time.time()
        for _ in range(100):
            if test_list:
                test_list.pop(0)
        list_pop_time = time.time() - start
        
        # æ¸¬è©¦ 2 - deque popleft - é«˜æ•ˆ
        test_deque = deque(range(size))
        start = time.time()
        for _ in range(100):
            if test_deque:
                test_deque.popleft()
        deque_pop_time = time.time() - start
        
        result = {
            'test': 'list_operations',
            'size': size,
            'metrics': {
                'list_pop_ms': list_pop_time * 1000,
                'deque_popleft_ms': deque_pop_time * 1000,
                'efficiency_improvement_percent': ((list_pop_time - deque_pop_time) / list_pop_time) * 100,
            }
        }
        
        return result
    
    async def run_all_benchmarks(self) -> Dict:
        """é‹è¡Œæ‰€æœ‰åŸºæº–æ¸¬è©¦"""
        print("\nğŸš€ é–‹å§‹æ•ˆèƒ½åŸºæº–æ¸¬è©¦...\n")
        
        tests = [
            ("æ—¥èªŒæ€§èƒ½", self.benchmark_logging(1000)),
            ("JSON åºåˆ—åŒ–", self.benchmark_json_serialization(1000)),
            ("å…§å­˜æ“ä½œ", self.benchmark_memory_operations(10000)),
            ("å­—ç¬¦ä¸²æ“ä½œ", self.benchmark_string_operations(10000)),
            ("åˆ—è¡¨æ“ä½œ", self.benchmark_list_operations(10000)),
        ]
        
        for test_name, test_coro in tests:
            print(f"â³ æ¸¬è©¦: {test_name}...")
            result = await test_coro
            self.results['tests'][test_name] = result
            print(f"âœ… å®Œæˆ: {test_name}\n")
        
        return self.results
    
    def print_summary(self) -> None:
        """æ‰“å°æ‘˜è¦å ±å‘Š"""
        print("\n" + "="*80)
        print("ğŸ“Š æ•ˆèƒ½åŸºæº–æ¸¬è©¦å ±å‘Š")
        print("="*80 + "\n")
        
        for test_name, test_data in self.results.get('tests', {}).items():
            print(f"\nğŸ” {test_name}")
            print("-" * 80)
            
            metrics = test_data.get('metrics', {})
            for metric_name, value in metrics.items():
                if 'percent' in metric_name.lower():
                    print(f"   {metric_name}: {value:.2f}%")
                elif 'ms' in metric_name or 'us' in metric_name:
                    print(f"   {metric_name}: {value:.4f}")
                else:
                    print(f"   {metric_name}: {value}")
        
        print("\n" + "="*80)
        print("âœ… åŸºæº–æ¸¬è©¦å®Œæˆ")
        print("="*80 + "\n")
    
    def save_results(self, filename: str = "benchmark_results.json") -> None:
        """ä¿å­˜çµæœåˆ° JSON æ–‡ä»¶"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        print(f"âœ… çµæœå·²ä¿å­˜åˆ° {filename}")


async def main():
    """ä¸»å‡½æ•¸"""
    benchmark = PerformanceBenchmark()
    
    try:
        await benchmark.run_all_benchmarks()
        benchmark.print_summary()
        benchmark.save_results()
    except Exception as e:
        print(f"âŒ åŸºæº–æ¸¬è©¦å‡ºéŒ¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())
